{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import math\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import signal\n",
    "from scipy.fft import fftshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: ../../data/models/enter_v1/model\\{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m LOAD_MODEL_PATH \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../../data/models/enter_v1\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# LOAD_MODEL_PATH = f'../../data/models/leave_v1'\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mLOAD_MODEL_PATH\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/model\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      5\u001B[0m params \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mLOAD_MODEL_PATH\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/params.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      6\u001B[0m history \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mLOAD_MODEL_PATH\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/history.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py:115\u001B[0m, in \u001B[0;36mparse_saved_model\u001B[1;34m(export_dir)\u001B[0m\n\u001B[0;32m    113\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot parse file \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_to_pbtxt\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mstr\u001B[39m(e)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 115\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(\n\u001B[0;32m    116\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSavedModel file does not exist at: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexport_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mos\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    117\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m{{\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mconstants\u001B[38;5;241m.\u001B[39mSAVED_MODEL_FILENAME_PBTXT\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m|\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    118\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconstants\u001B[38;5;241m.\u001B[39mSAVED_MODEL_FILENAME_PB\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m}}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mOSError\u001B[0m: SavedModel file does not exist at: ../../data/models/enter_v1/model\\{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "LOAD_MODEL_PATH = f'../../data/models/enter_v1'\n",
    "# LOAD_MODEL_PATH = f'../../data/models/leave_v1'\n",
    "\n",
    "model = keras.models.load_model(f'{LOAD_MODEL_PATH}/model')\n",
    "params = pd.read_csv(f'{LOAD_MODEL_PATH}/params.csv')\n",
    "history = pd.read_csv(f'{LOAD_MODEL_PATH}/history.csv')\n",
    "HISTORY_SIZE = params['history_size'][0]\n",
    "HORIZON_SIZE = params['horizon_size'][0]\n",
    "# leave_std = params['y_std'][0]\n",
    "enter_std = params['y_std'][0]\n",
    "search_std = params['search_std'][0]\n",
    "\n",
    "data_norm = pd.read_csv(f'{LOAD_MODEL_PATH}/dataset.csv', parse_dates=['dt']).set_index('dt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enter_test = pd.read_csv('../../data/marking/enter-test-aggregate.csv', parse_dates=['dt'], index_col='dt')\n",
    "enter_test_smooth = enter_test.rolling(7, center=True).mean()\n",
    "enter_test_smooth['enter_cnt_test'] = enter_test_smooth['enter_cnt']\n",
    "del enter_test_smooth['enter_cnt']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "px.line(history, y=['loss', 'val_loss'], log_y=True, log_x=True).show()\n",
    "px.line(history, y=['accuracy', 'val_accuracy'], log_y=True, log_x=True).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_shape = (len(data_norm.columns), HISTORY_SIZE)\n",
    "x_elm = x_shape[0] * x_shape[1]\n",
    "y_shape = (1, HORIZON_SIZE)\n",
    "y_elm = y_shape[0] * y_shape[1]\n",
    "\n",
    "def test_model(offset):\n",
    "    out = data_norm.copy()\n",
    "    tx = out.copy().iloc[len(data_norm) - offset - HISTORY_SIZE:len(data_norm) - offset]\n",
    "    # tx['enter_cnt'] /= enter_std\n",
    "    # tx['leave_cnt'] /= leave_std\n",
    "    # tx['search_cnt'] /= search_std\n",
    "    mx = tx.values.reshape((1, x_elm))\n",
    "    my = model.predict(mx)\n",
    "    my = my.reshape(y_shape).transpose()\n",
    "    ty = pd.DataFrame(my)\n",
    "    ty_index = pd.date_range(tx.index[-1] + pd.DateOffset(1), tx.index[-1] + pd.DateOffset(HORIZON_SIZE), freq='D')\n",
    "    ty = pd.DataFrame({ 'dt': ty_index, 'enter_cnt': ty[0] }).set_index('dt')\n",
    "    # ty = pd.DataFrame({ 'dt': ty_index, 'leave_cnt': ty[0] }).set_index('dt')\n",
    "    ty['enter_cnt'] *= enter_std\n",
    "    out['enter_cnt'] *= enter_std\n",
    "    out = pd.concat([out, enter_test_smooth])\n",
    "    out = out.merge(ty, on='dt', how='outer', suffixes=['', '_model'])\n",
    "\n",
    "    res = out\n",
    "    px.line(res, y=['enter_cnt', 'enter_cnt_model', 'enter_cnt_test'], title=f\"offset = {offset}\").show()\n",
    "    # px.line(res, y=['leave_cnt', 'leave_cnt_model'], title=f\"offset = {offset}\").show()\n",
    "\n",
    "test_model(0*7)\n",
    "test_model(5*7)\n",
    "test_model(10*7)\n",
    "test_model(15*7)\n",
    "test_model(20*7)\n",
    "test_model(25*7)\n",
    "test_model(30*7)\n",
    "test_model(35*7)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
